## Also see https://github.com/sreetamadas/web_links/blob/master/3_model_explainability.txt  ##

PROBLEM STATEMENT: Can we use GradCAM to show the defective region (as predicted by the model) on the image, when using classification models?


“region of interest highlighting” – is this valid for multi-label classification? 
Reference https://www.groundai.com/project/smooth-grad-cam-an-enhanced-inference-level-visualization-technique-for-deep-convolutional-neural-network-models/1  . 
It is built on top of last convolution layer before predicting the layers



GradCAM can be used with any CNN based architecture irrespective of the task. 
Their official page has demo links for image captioning and visual question answering as well . (http://gradcam.cloudcv.org/)
 What GradCAM tries to show is - how intensely the input image activates different channels by how important each channel is with regard to the class

The advantage of GradCAM is that, its predecessor – CAM works only for specific CNN architectures (which have global average pooling over convolutional maps immediately before prediction) whereas GradCAM works for any CNN architecture

ref: 
https://towardsdatascience.com/demystifying-convolutional-neural-networks-using-gradcam-554a85dd4e48
https://www.hackevolve.com/where-cnn-is-looking-grad-cam/

https://github.com/sicara/tf-explain



SAMPLE DATA: NEU dataset


TASKS:
need to set up code to run with test images, which are not part of imagedatabunch
output should show the test images with the heat map
compare this to raw test images with ground truth (GT) bounding box



https://towardsdatascience.com/anomaly-detection-in-images-777534980aeb  (keras)
https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/ 
https://www.kaggle.com/daisukelab/verifying-cnn-models-with-cam-and-etc-fast-ai 
google "activation map in fastai" - throws up relevant links


https://github.com/anurags25/FastAI-LIME
https://github.com/marcotcr/lime/issues/374
https://github.com/anurags25/FastAI-LIME/blob/master/LIME-Pets.ipynb
https://github.com/anurags25/FastAI-LIME/blob/master/lime_image.py
check if this implementation works https://github.com/jacobgil/pytorch-grad-cam 
https://github.com/yiskw713/SmoothGradCAMplusplus 
The current working implementation: https://www.kaggle.com/daisukelab/verifying-cnn-models-with-cam-and-etc-fast-ai


this blog is trying separate maps for each of the classes https://towardsdatascience.com/multi-label-classification-and-class-activation-map-on-fashion-mnist-1454f09f5925
Another link https://github.com/mingming97/multilabel-cam   this is marked as multi-label - will need to check if this works
https://glassboxmedicine.com/2020/05/29/grad-cam-visual-explanations-from-deep-networks/   says we should be able to use this for multiple classes
 

general reading:
https://forums.fast.ai/t/is-there-a-generic-heatmap-grad-cam-api/57218/2
https://forums.fast.ai/t/using-class-activation-maps-cam-for-visualization/18864
https://jacobgil.github.io/deeplearning/class-activation-maps
https://medium.com/@mrsalehi/a-review-of-different-interpretation-methods-in-deep-learning-part-1-saliency-map-cam-grad-cam-3a34476bc24d
https://medium.com/@mrsalehi/a-review-of-different-interpretation-methods-in-deep-learning-part-2-input-gradient-layerwise-e077609b6377 


